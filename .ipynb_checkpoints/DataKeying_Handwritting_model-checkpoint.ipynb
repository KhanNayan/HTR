{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a798e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# from google.colab import drive #To use googel drive to get files.\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.activations import relu, sigmoid, softmax\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6674c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data_keying_data/img_labels.txt') as f:\n",
    "    contents = f.readlines()\n",
    "\n",
    "lines = [line.strip() for line in contents] \n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c685d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_label_len = 0\n",
    "\n",
    "char_list = \"!\\\"#&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\" \n",
    "\n",
    "# string.ascii_letters + string.digits (Chars & Digits)\n",
    "# or \n",
    "# \"!\\\"#&'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "print(char_list, len(char_list))\n",
    "\n",
    "def encode_to_labels(txt):\n",
    "    # encoding each output word into digits\n",
    "    dig_lst = []\n",
    "    for index, chara in enumerate(txt):\n",
    "        dig_lst.append(char_list.index(chara))\n",
    "        \n",
    "    return dig_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76fe709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "RECORDS_COUNT = 100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf28d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = []\n",
    "train_labels = []\n",
    "train_input_length = []\n",
    "train_label_length = []\n",
    "train_original_text = []\n",
    "\n",
    "valid_images = []\n",
    "valid_labels = []\n",
    "valid_input_length = []\n",
    "valid_label_length = []\n",
    "valid_original_text = []\n",
    "\n",
    "inputs_length = []\n",
    "labels_length = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed9eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_removal(img):\n",
    "    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10,3))\n",
    "    vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,10))\n",
    "    rows = []\n",
    "    col = []\n",
    "#     file_path = path\n",
    "#     image = cv2.imread(file_path)\n",
    "#     gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    gray = img\n",
    "    value,thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    horizontal_th, horizontal_dst = cv2.threshold(gray, value+10, 255, cv2.THRESH_BINARY_INV)\n",
    "    vertical_th, vertical_dst = cv2.threshold(gray, value, 255, cv2.THRESH_BINARY_INV)\n",
    "    horizontal_lines = cv2.dilate(horizontal_dst, horizontal_kernel , iterations=1)\n",
    "    vertical_lines = cv2.dilate(vertical_dst, vertical_kernel , iterations=1)\n",
    "    row,column = thresh.shape\n",
    "    for i in range(row):\n",
    "        if np.all(horizontal_lines[i]==255):\n",
    "            rows.append(i)\n",
    "\n",
    "    for i in range(column):\n",
    "        if np.all(vertical_lines[:,i]==255):\n",
    "            col.append(i)\n",
    "\n",
    "    copy_col = col[:]\n",
    "    count = 0\n",
    "    first = -1\n",
    "    for i in range(len(copy_col)):\n",
    "        if i-1<0:\n",
    "            continue\n",
    "        elif copy_col[i]-1 == copy_col[i-1]:\n",
    "            \n",
    "            if first == -1:\n",
    "                first = i-1\n",
    "            count = count+1\n",
    "        else:\n",
    "            if count>=20:\n",
    "                del col[first:i]\n",
    "            count = 0\n",
    "            first = -1\n",
    "    if count>=20:\n",
    "        del col[first:i+1]\n",
    "    count = 0\n",
    "    for i in range(len(col)):\n",
    "        thresh = np.delete(thresh,col[i]-count,1)\n",
    "        count = count+1\n",
    "    count = 0\n",
    "    for i in range(len(rows)):\n",
    "        thresh = np.delete(thresh,rows[i]-count,0)\n",
    "        count = count+1\n",
    "    thresh = cv2.bitwise_not(thresh)\n",
    "\n",
    "    return thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586f4296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img):\n",
    "    \"\"\"\n",
    "    Converts image to shape (32, 128, 1) & normalize\n",
    "    \"\"\"\n",
    "    w, h = img.shape\n",
    "    \n",
    "#     _, img = cv2.threshold(img, \n",
    "#                            128, \n",
    "#                            255, \n",
    "#                            cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Aspect Ratio Calculation\n",
    "    new_w = 32\n",
    "    new_h = int(h * (new_w / w))\n",
    "    img = cv2.resize(img, (new_h, new_w))\n",
    "    w, h = img.shape\n",
    "    \n",
    "    img = img.astype('float32')\n",
    "    \n",
    "    # Converts each to (32, 128, 1)\n",
    "    if w < 32:\n",
    "        add_zeros = np.full((32-w, h), 255)\n",
    "        img = np.concatenate((img, add_zeros))\n",
    "        w, h = img.shape\n",
    "    \n",
    "    if h < 128:\n",
    "        add_zeros = np.full((w, 128-h), 255)\n",
    "        img = np.concatenate((img, add_zeros), axis=1)\n",
    "        w, h = img.shape\n",
    "        \n",
    "    if h > 128 or w > 32:\n",
    "        dim = (128,32)\n",
    "        img = cv2.resize(img, dim)\n",
    "    \n",
    "    img = cv2.subtract(255, img)\n",
    "    \n",
    "    img = np.expand_dims(img, axis=2)\n",
    "    \n",
    "    # Normalize \n",
    "    img = img / 255\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebf9c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, line in enumerate(lines):\n",
    "    splits = line.split(' ')\n",
    "    if len(splits) == 3:\n",
    "        word = splits[1] + ' ' + splits[2]\n",
    "#         print(word)\n",
    "    else:\n",
    "        word = splits[1]\n",
    "    \n",
    "    filepath = 'Data_keying_data/img_data/' + splits[0] + '.jpg'\n",
    "    print(filepath)\n",
    "\n",
    "    img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "    try:\n",
    "        img = line_removal(img)\n",
    "        img = process_image(img)\n",
    "    except:\n",
    "        continue\n",
    "#     print(img)\n",
    "    # processing on label\n",
    "    try:\n",
    "        label = encode_to_labels(word)\n",
    "    except:\n",
    "        continue    \n",
    "#     print(label)\n",
    "    if index % 10 == 0:\n",
    "        valid_images.append(img)\n",
    "        valid_labels.append(label)\n",
    "        valid_input_length.append(31)\n",
    "        valid_label_length.append(len(word))\n",
    "        valid_original_text.append(word)\n",
    "#     if index % 10 == 0:\n",
    "#         train_images.append(img)\n",
    "#         train_labels.append(label)\n",
    "#         train_input_length.append(31)\n",
    "#         train_label_length.append(len(word))\n",
    "#         train_original_text.append(word)\n",
    "    else:\n",
    "        train_images.append(img)\n",
    "        train_labels.append(label)\n",
    "        train_input_length.append(31)\n",
    "        train_label_length.append(len(word))\n",
    "        train_original_text.append(word)        \n",
    "\n",
    "    if len(word) > max_label_len:\n",
    "        max_label_len = len(word)\n",
    "\n",
    "    if index >= RECORDS_COUNT:\n",
    "        break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b3d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, line in enumerate(lines):\n",
    "#     splits = line.split(' ')\n",
    "#     status = splits[1]\n",
    "    \n",
    "#     if status == 'ok':\n",
    "#         word_id = splits[0]\n",
    "#         word = \"\".join(splits[8:])\n",
    "        \n",
    "#         splits_id = word_id.split('-')\n",
    "#         filepath = 'words/{}/{}-{}/{}.png'.format(splits_id[0], \n",
    "#                                                   splits_id[0], \n",
    "#                                                   splits_id[1], \n",
    "#                                                   word_id)\n",
    "        \n",
    "#         # processing on image\n",
    "#         img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "#         try:\n",
    "#             img = process_image(img)\n",
    "#         except:\n",
    "#             continue\n",
    "#         print(img) \n",
    "#         # processing on label\n",
    "#         try:\n",
    "#             label = encode_to_labels(word)\n",
    "#         except:\n",
    "#             continue\n",
    "        \n",
    "#         if index % 10 == 0:\n",
    "#             valid_images.append(img)\n",
    "#             valid_labels.append(label)\n",
    "#             valid_input_length.append(31)\n",
    "#             valid_label_length.append(len(word))\n",
    "#             valid_original_text.append(word)\n",
    "#         else:\n",
    "#             train_images.append(img)\n",
    "#             train_labels.append(label)\n",
    "#             train_input_length.append(31)\n",
    "#             train_label_length.append(len(word))\n",
    "#             train_original_text.append(word)\n",
    "        \n",
    "#         if len(word) > max_label_len:\n",
    "#             max_label_len = len(word)\n",
    "    \n",
    "#     if index >= RECORDS_COUNT:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_padded_label = pad_sequences(train_labels, \n",
    "                             maxlen=max_label_len, \n",
    "                             padding='post',\n",
    "                             value=len(char_list))\n",
    "\n",
    "valid_padded_label = pad_sequences(valid_labels, \n",
    "                             maxlen=max_label_len, \n",
    "                             padding='post',\n",
    "                             value=len(char_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91083bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded_label.shape, valid_padded_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c50a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.asarray(train_images)\n",
    "train_input_length = np.asarray(train_input_length)\n",
    "train_label_length = np.asarray(train_label_length)\n",
    "valid_images = np.asarray(valid_images)\n",
    "valid_input_length = np.asarray(valid_input_length)\n",
    "valid_label_length = np.asarray(valid_label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aba720",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32759807",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(valid_images[0].reshape(32,128), cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f3953c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input with shape of height=32 and width=128 \n",
    "inputs = Input(shape=(32,128,1))\n",
    " \n",
    "# convolution layer with kernel size (3,3)\n",
    "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
    "# poolig layer with kernel size (2,2)\n",
    "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
    " \n",
    "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
    "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
    " \n",
    "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
    " \n",
    "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
    "\n",
    "# poolig layer with kernel size (2,1)\n",
    "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
    " \n",
    "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
    "\n",
    "# Batch normalization layer\n",
    "batch_norm_5 = BatchNormalization()(conv_5)\n",
    " \n",
    "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
    "batch_norm_6 = BatchNormalization()(conv_6)\n",
    "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
    " \n",
    "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
    " \n",
    "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
    " \n",
    "# bidirectional LSTM layers with units=128\n",
    "blstm_1 = Bidirectional(LSTM(256, return_sequences=True, dropout = 0.2))(squeezed)\n",
    "blstm_2 = Bidirectional(LSTM(256, return_sequences=True, dropout = 0.2))(blstm_1)\n",
    " \n",
    "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
    "\n",
    "# model to be used at test time\n",
    "act_model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6f41ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfef42b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "the_labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    \n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, the_labels, input_length, label_length])\n",
    "\n",
    "#model to be used at training time\n",
    "model = Model(inputs=[inputs, the_labels, input_length, label_length], outputs=loss_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db9ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 20\n",
    "e = str(epochs)\n",
    "optimizer_name = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c40d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = optimizer_name, metrics=['accuracy'])\n",
    "\n",
    "filepath=\"Model/adam_16Nov.hdf5\".format(optimizer_name,\n",
    "                                          str(RECORDS_COUNT),\n",
    "                                          str(epochs),\n",
    "                                          str(train_images.shape[0]),\n",
    "                                          str(valid_images.shape[0]))\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='loss', verbose=1, save_best_only=True, mode='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f8850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=[train_images, train_padded_label, train_input_length, train_label_length],\n",
    "                    y=np.zeros(len(train_images)),\n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs, \n",
    "                    validation_data=([valid_images, valid_padded_label, valid_input_length, valid_label_length], [np.zeros(len(valid_images))]),\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved best model weights\n",
    "act_model.load_weights(filepath)\n",
    "\n",
    "# predict outputs on validation images\n",
    "prediction = act_model.predict(valid_images)\n",
    " \n",
    "# use CTC decoder\n",
    "decoded = K.ctc_decode(prediction, \n",
    "                       input_length=np.ones(prediction.shape[0]) * prediction.shape[1],\n",
    "                       greedy=True)[0][0]\n",
    "out = K.get_value(decoded)\n",
    "\n",
    "import Levenshtein as lv\n",
    "\n",
    "total_jaro = 0\n",
    "total_rati = 0\n",
    "# see the results\n",
    "for i, x in enumerate(out):\n",
    "    letters=''\n",
    "    for p in x:\n",
    "        if int(p) != -1:\n",
    "            letters+=char_list[int(p)]\n",
    "    total_jaro+=lv.jaro(letters, valid_original_text[i])\n",
    "    total_rati+=lv.ratio(letters, valid_original_text[i])\n",
    "\n",
    "print('jaro :', total_jaro/len(out))\n",
    "print('ratio:', total_rati/len(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa41fab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction =act_model.predict(valid_images)\n",
    " \n",
    "# use CTC decoder\n",
    "decoded = K.ctc_decode(prediction,   \n",
    "                       input_length=np.ones(prediction.shape[0]) * prediction.shape[1],\n",
    "                       greedy=True)[0][0]\n",
    "\n",
    "out = K.get_value(decoded)\n",
    "\n",
    "# see the results\n",
    "for i, x in enumerate(out):\n",
    "    print(\"original_text =  \", valid_original_text[i])\n",
    "    print(\"predicted text = \", end = '')\n",
    "    for p in x:\n",
    "        if int(p) != -1:\n",
    "            print(char_list[int(p)], end = '')\n",
    "    plt.imshow(valid_images[i].reshape(32,128), cmap=plt.cm.gray)\n",
    "    plt.show()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1cb73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829f003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b41c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction =act_model.predict(test_data)\n",
    " \n",
    "# # use CTC decoder\n",
    "# decoded = K.ctc_decode(prediction,   \n",
    "#                        input_length=np.ones(prediction.shape[0]) * prediction.shape[1],\n",
    "#                        greedy=True)[0][0]\n",
    "\n",
    "# out = K.get_value(decoded)\n",
    "\n",
    "# # see the results\n",
    "# for i, x in enumerate(out):\n",
    "    \n",
    "#     print(\"predicted text = \", end = '')\n",
    "#     for p in x:\n",
    "#         if int(p) != -1:\n",
    "#             print(char_list[int(p)], end = '')\n",
    "#     plt.imshow(test_data[i].reshape(32,128), cmap=plt.cm.gray)\n",
    "#     plt.show()\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Model/16Nov.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e026c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_model.save('Model/act_16Nov.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
